name='MLP-basica-logistic'
batch_normalization='OFF'
batch_size=2
learning_rate=0.003
activation='tanh'
init_mode='glorot_uniform'
n_neurons=10
n_hidden_layers=1